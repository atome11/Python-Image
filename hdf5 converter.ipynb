{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hdf5 converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "\n",
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = r'C:\\Users\\DevineauY\\Documents\\Audit\\2019-Contract Management\\2-Data sources\\Signature\\dataset.hdf5'  # address to save the hdf5 file\n",
    "signature_train_path = r'C:\\Users\\DevineauY\\Documents\\Audit\\2019-Contract Management\\2-Data sources\\Signature\\*.png'\n",
    "\n",
    "# read addresses and labels from the 'train' folder\n",
    "addrs = glob.glob(signature_train_path)\n",
    "\n",
    "labels = []\n",
    "for addr in addrs:\n",
    "    if 'foot' in addr: # signature footer\n",
    "        labels.append(0)\n",
    "    elif 'full' in addr: # signature main\n",
    "        labels.append(1)\n",
    "    elif 'FALSE' not in addr: # date handwritten\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(3)\n",
    "\n",
    "# to shuffle data\n",
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)\n",
    "    \n",
    "# Divide the data into 60% train, 20% validation, and 20% test\n",
    "train_addrs = addrs[0:int(0.9*len(addrs))]\n",
    "train_labels = labels[0:int(0.9*len(labels))]\n",
    "\n",
    "#val_addrs = addrs[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "#val_labels = labels[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "\n",
    "test_addrs = addrs[int(0.9*len(addrs)):]\n",
    "test_labels = labels[int(0.9*len(labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "w = 64\n",
    "h = 64\n",
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    train_shape = (len(train_addrs), 3, w, h)\n",
    "    #val_shape = (len(val_addrs), 3, w, h)\n",
    "    test_shape = (len(test_addrs), 3, w, h)\n",
    "elif data_order == 'tf':\n",
    "    train_shape = (len(train_addrs), w, h, 3)\n",
    "    #val_shape = (len(val_addrs), w, h, 3)\n",
    "    test_shape = (len(test_addrs), w, h, 3)\n",
    "    \n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "\n",
    "hdf5_file.create_dataset(\"train_img\", train_shape, np.uint8)\n",
    "#hdf5_file.create_dataset(\"val_img\", val_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"test_img\", test_shape, np.uint8)\n",
    "\n",
    "hdf5_file.create_dataset(\"train_mean\", train_shape[1:], np.float32)\n",
    "\n",
    "hdf5_file.create_dataset(\"train_labels\", (len(train_addrs),), np.uint8)\n",
    "hdf5_file[\"train_labels\"][...] = train_labels\n",
    "\n",
    "#hdf5_file.create_dataset(\"val_labels\", (len(val_addrs),), np.int8)\n",
    "#hdf5_file[\"val_labels\"][...] = val_labels\n",
    "\n",
    "hdf5_file.create_dataset(\"test_labels\", (len(test_addrs),), np.uint8)\n",
    "hdf5_file[\"test_labels\"][...] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "def feed_dataset_h5(addrs,img_label, w, h):\n",
    "    \"\"\"\n",
    "    Process the images and resize them to feed the data set \n",
    "    \n",
    "    Arguments:\n",
    "    dataset -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "\n",
    "    Returns:\n",
    "     -- \n",
    "    \"\"\"\n",
    "    for i in range(len(addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print('{} data: {}/{}'.format(img_label, i, len(addrs)))\n",
    "\n",
    "        # read an image and resize to (w, h)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = addrs[i]\n",
    "        img = Image.open(addr)\n",
    "        img = img.resize((w,h))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # add any image pre-processing here\n",
    "        return img\n",
    "        # save the image and calculate the mean so far\n",
    "        #hdf5_file[img_label][i, ...] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feed_dataset_h5(addrs,img_label, w, h):\n",
    "    \"\"\"\n",
    "    Process the images and resize them to feed the data set \n",
    "    \n",
    "    Arguments:\n",
    "    dataset -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "\n",
    "    Returns:\n",
    "     -- \n",
    "    \"\"\"\n",
    "    for i in range(len(addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "        if i % 50 == 0 and i > 1:\n",
    "            print('{} data: {}/{}'.format(img_label, i, len(addrs)))\n",
    "\n",
    "        # read an image and resize to (w, h)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = addrs[i]\n",
    "        img = cv2.imread(addr, cv2.IMREAD_UNCHANGED)\n",
    "        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # add any image pre-processing here\n",
    "        #plt.imshow(img)\n",
    "        # save the image and calculate the mean so far\n",
    "        hdf5_file[img_label][i, ...] = img[None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img data: 50/434\n",
      "train_img data: 100/434\n",
      "train_img data: 150/434\n",
      "train_img data: 200/434\n",
      "train_img data: 250/434\n",
      "train_img data: 300/434\n",
      "train_img data: 350/434\n",
      "train_img data: 400/434\n"
     ]
    }
   ],
   "source": [
    "feed_dataset_h5(train_addrs, \"train_img\", w, h)\n",
    "#feed_dataset_h5(val_addrs, \"val_img\", w, h)\n",
    "feed_dataset_h5(test_addrs, \"test_img\", w, h)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hdf5 file\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "\n",
    "# Total number of samples\n",
    "data_num = hdf5_file[\"train_img\"].shape[0]\n",
    "#val_num = hdf5_file[\"val_img\"].shape[0]\n",
    "test_num = hdf5_file[\"test_img\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 49)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num,test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cfd9fb016e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# create list of batches to shuffle the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbatches_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create list of batches to shuffle the data\n",
    "batches_list = list(range(int(ceil(float(data_num) / batch_size))))\n",
    "shuffle(batches_list)\n",
    "\n",
    "# loop over batches\n",
    "for n, i in enumerate(batches_list):\n",
    "    i_s = i * batch_size  # index of the first image in this batch\n",
    "    i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "    \n",
    "    # read batch images and remove training mean\n",
    "    images = hdf5_file.root.train_img[i_s:i_e]\n",
    "    if subtract_mean:\n",
    "        images -= mm\n",
    "        \n",
    "    # read labels and convert to one hot encoding\n",
    "    labels = hdf5_file.root.train_labels[i_s:i_e]\n",
    "    labels_one_hot = np.zeros((batch_size, nb_class))\n",
    "    labels_one_hot[np.arange(batch_size), labels] = 1\n",
    "    print(n+1, '/', len(batches_list))\n",
    "    print(labels[0], labels_one_hot[0, :])\n",
    "    plt.imshow(images[0])\n",
    "    plt.show()\n",
    "    \n",
    "    if n == 5:  # break after 5 batches\n",
    "        break\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
